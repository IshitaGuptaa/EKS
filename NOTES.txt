I AM USING RHEL 7.5 for my work

STEP1:  Create AWS account, then from root account(default) make one user profile , give admin property or give it eksclusterPolicy, save the credentials. 
STEP2: aws-cli download 
	command: curl "https://s3.amazonaws.com/aws-cli/awscli-bundle.zip" -o "awscli-bundle.zip"
 
	command: unzip awscli-bundle.zip

	command: sudo ./awscli-bundle/install -i /usr/local/aws -b /usr/local/bin/aws

	command: sudo /usr/local/bin/python3.7 awscli-bundle/install -i /usr/local/aws -b /usr/local/bin/aws
	
	command: aws --version (Check once)

 
STEP3: aws configure , Enter Access key, private key and region as is mentined in the cred file, we saved.
STEP4: for eks we can either use aws eks, but it is very tedious to handle. We have another alternate present that provides better functionality, ease and more advance features viz "eksctl"
	
	command: curl --silent --location "https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz" | tar xz -C /tmp

	command: sudo mv /tmp/eksctl /usr/local/bin

	command: eksctl version (Check once)

STEP5: apply cluster.yml, this will call cloud formation in backgroud (teraform like AWS feature), 
 	
	command: eksctl create cluster -f cluster.yml

// To store the output in a file for later reference we can also use tee command
	command: eksctl create cluster -f cluster.yml | tee output.txt

STEP5: You can check the cluster-info and config view of kubectl using following commad

	command: kubectl config view
	command: kubectl cluster-info

STEP6: update configuration of aws using follwoing command

	commad: aws update-kubeconfig --name IishitaCluster1 
Configures kubectl so that you can connect to an Amazon EKS cluster.

STEP7: Create your own namespace

	command: kubectl create namespace eksdemo
note: the namespace should be all small letters

STEP8: Now in the config file we can also setup what namespace we will be using by default, we set this info in the context block, either we go inside config file and change it manually or we use cli command

	command: kubectl config set-context --current --namespace=eksdemo
// That is in the current context set this namespace as default.

STEP9: Now our basic setup is ready , but let's make it more efficient. We can add spo-instance in our system. which will be like a alarm or metric on instance, such that if instance exceeds certain limit mentioned , then it will either stop, terminate or hibernate the system. This is very cost efficient.


{NOTE:  TO DELETE CLUSTER USE     "eksctl delete cluster --region=ap-south-1 --name=IshitaCluster1", DELETE SSH KEYS MANUALLY IN AWS WEB UI, DELETE VPC,SECURITY GROUPS, ETC.. IF NOT DELETED}
























STEP10: 
FARGATE CLUSTER-

In case user wants to create serverless architecture he/she can use Fargate. 

	command: eksctl create cluster -f fcluster.yml
Note: We are launching fargate cluster in Singapore coz Mumbai doesn't support Fargate


NOTE: Update amazon eks config file
	command: aws update-kubeconfig --name FargateIshitaCluster1

NOTE - Now that we have launched our cluster we can make our nameserver and use it as "-n ns1(say)" everytime we are lauching any depolyment or anything or we can make this nameserver as default in our cluster config file
	command: kubectl create namespace eksdemo
	command: kubectl config set-context --current --namespace=eksdemo



STEP 11: Creating EFS deployent

As one Pod goes down EKS can launch pod in any Availability domain or zone (AD or AZ)(because, our cluster uses multiple AZ for lault tolernace), this is good for FT , but the problem is with the EBS, when the POD launches in another AD we don not have any EBS attached to it, which causes data loss. Though EBS is still present in our resouces but we to attach the POD and the EBS should be in same AZ.

So we use the efs deployment, to make data peristent at all times. Even if EFS goes down , deployment will make sure it's always up and running.

	command: kubectl apply -f deployment_EFS_provisioner.yml

// You should change the env variables for using it without error.

STEP 12: creating RBAC

now note, we as a user uses login and password to connect with the fargate cluster, but the cluster uses Role based on IAM policy, which allows it to interact with other services. 


USER---login_cred---->>CLUSTER----rbac---->>AMAZON SERVICES

	command: kubectl apply -f RBAC.yml

File Explanation:

Subjects: The set of users and processes that want to access the Kubernetes API.

RoleBinding: Will connect the remaining entity-subjects. Given a role, which already binds API Objects and verbs, we will establish which subjects can use it. For the cluster-level, non-namespaced equivalent, there are ClusterRoleBindings.

Users: These are global, and meant for humans or processes living outside the cluster.

ServiceAccounts: These are namespaced and meant for intra-cluster processes running inside pods.

name in subjects: The user or profile name you want to give access to (if kind is user), if kind is service Account, then it matters, we use default as default

RoleRef: which role are we reffering to
kind: two types Role or ClusterRole (R capital)
name: name of role or Clusterrole



It's important to mention the Api group



STEP13: Creating Storage Class and PVCs

In storage class we need to mention , the provisioner. EBS or EFS.
NOTE- In case of EBS, kubernetes can handle the provisioning. It can go to aws, create EBS based on our requirement, like Volume size or type. 
But in case of EFS, we are using our deployment as provisioner. This EFS deployment manages the EFS resouces for us, the same way kubernets.io does for EBS, but kubernetes.io doesn't support EBS. so we use EFS deployment to make the data persitent covering all factors


NOTE: in pvc to link it with sc, we can either use StorageClassName or annotions:volume.beta.kubernetes.io/storage-class. Annotion is outdated,still working but will be removed in coming kubernetes releases



STEP14: amazon-efs utils download
	command: sudo yum -y install git
	command: git clone https://github.com/aws/efs-utils
	command: sudo yum -y install make
    











STEP-10: Like yum we have helm in kubernetes that acts as repo. We have helm hub like Docker hub or GitHub. Like linux we have software  in Helm we have charts 

It has two componenets viz Tiller(Server) and Helm(Client)

Download and instal Tiller and Helm softwares
	
	command: wget https://get.helm.sh/helm-v2.16.9-linux-amd64.tar.gz
	command: tar -zxvf  helm-v2.16.9-linux-amd64.tar.gz
	command: mv linux-amd64/helm /usr/local/bin/helm

STEP 11: Now we will setup account and put tiller in use.

	command: kubectl -n kube-system create serviceaccount tiller
	command: kubectl create clusterrolebinding tiller --clusterrole cluster-admin --serviceaccount=kube-system:tiller
	command: helm init --service-account tiller --upgrade




STEP{}: Let's create our separate namespace for prometheus and garafana for monitoringa nd better managemnet, also since they are a part of private cluster IP we are going to do the port forwarding, so for better management and security, better to put it in separate namespace. 
Note - there is difference in Port forwarding and normal pod expose. 
We use Expose for DMZ pods and port forwarding for the one with CLuster IP. Port forwading is temporary and thus we need to run the command after every reboot and is mostly recommended for devlopment or troubleshooting phase and not for production env.

	command: kubectl create namespace prometheus

	command: helm install  stable/prometheus     --namespace prometheus     --set alertmanager.persistentVolume.storageClass="gp2"     --set server.persistentVolume.storageClass="gp2"

	command kubectl get svc -n prometheus // to check

	command: kubectl -n prometheus  port-forward svc/flailing-buffalo-prometheus-server  8888:80 //EXPOSE 
	



STEP{}: We'll do the same for garafana and use prometheus for it's data source. 
We will create our own workspace and that will give us the graphical presentation of our kubenetes functioning


	command: kubectl create namespace grafana

	command: helm install stable/grafana  --namespace grafana     --set persistence.storageClassName="gp2" --set adminPassword='GrafanaAdm!n'    --set datasources."datasources\.yaml".apiVersion=1     --set datasources."datasources\.yaml".datasources[0].name=Prometheus   --set datasources."datasources\.yaml".datasources[0].type=prometheus    --set datasources."datasources\.yaml".datasources[0].url=http://prometheus-server.prometheus.svc.cluster.local   --set datasources."datasources\.yaml".datasources[0].access=proxy     --set datasources."datasources\.yaml".datasources[0].isDefault=true  --set service.type=LoadBalancer


	command: kubectl get  secret  worn-bronco-grafana   --namespace  grafana  -o yaml



OR WE CAN USE YML FILES TO LAUNCH AND HANDLE PROMETHEUS AND GRAFANA



ADDITION INFO:

incase you encounter problem like "Signature expired: 20200724T112735Z is now earlier than 20200724T113039Z" , then sync the time with the sever usii=ng following commad.

	command:sudo ntpdate pool.ntp.org
Or you can create on cron file to do it daily automatically.

file-
#!/bin/sh
# sync server time
/usr/sbin/ntpdate pool.ntp.org >> /tmp/ntpdate.log

make it executable
chmod +x /etc/cron.daily/ntpdate

		

